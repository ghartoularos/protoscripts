{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein to Transcripts\n",
    "Quick script for gettin human coding transcripts from protein names,\n",
    "some of which may be Cluster of Differentiation (CD) names. If CD\n",
    "names are present, uses Uniprot-published CD->Uniprot \n",
    "conversion table (found at https://www.uniprot.org/docs/cdlist.txt).\n",
    "\n",
    "If script has been run once already, can use generated --c flag\n",
    "to use stored pickled dictionary of conversions.\n",
    "\n",
    "After converting all (if any) from CD names:\n",
    "1. Inputs Uniprot IDs or gene symbol/alias to mygene package \n",
    "(http://mygene.info/) to get associated ENSG IDs from Ensemble.\n",
    "2. Sses Ensembl API (http://rest.ensembl.org) to get all transcripts\n",
    "as ENST IDs associated with that ENSG ID. \n",
    "3. Then cross-checks for published list of protein-coding human \n",
    "transcripts from Gencode \n",
    "(https://www.gencodegenes.org/releases/28lift37.html).\n",
    "\n",
    "Script can be run both in Jupyter Notebook and on command line. Use \n",
    "\n",
    "`jupyter nbconvert --to=python protoscripts.ipynb`\n",
    "\n",
    "to convert this to a CLI script, and **update the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proteins = \"markerlist.txt\"\n",
    "pathtogencode = \"gencode.v28.pc_transcripts.fa.gz\"\n",
    "numbases = 'all'\n",
    "outputname = None\n",
    "CDpkl = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running on jupyter, **do not** run this next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "### Setup argument parser ###\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\"\"\n",
    "Quick script for gettin human coding transcripts from protein names,\n",
    "some of which may be Cluster of Differentiation (CD) names. If CD\n",
    "names are present, uses Uniprot-published CD->Uniprot \n",
    "conversion table (found at https://www.uniprot.org/docs/cdlist.txt).\n",
    "\n",
    "If script has been run once already, can use generated --c flag\n",
    "to use stored pickled dictionary of conversions.\n",
    "\n",
    "After converting all (if any) from CD names:\n",
    "1. Inputs Uniprot IDs or gene symbol/alias to mygene package \n",
    "(http://mygene.info/) to get associated ENSG IDs from Ensemble.\n",
    "2. Sses Ensembl API (http://rest.ensembl.org) to get all transcripts\n",
    "as ENST IDs associated with that ENSG ID. \n",
    "3. Then cross-checks for published list of protein-coding human \n",
    "transcripts from Gencode \n",
    "(https://www.gencodegenes.org/releases/28lift37.html).\n",
    "\"\"\", formatter_class=argparse.RawTextHelpFormatter)\n",
    "\n",
    "parser.add_argument('p', metavar='proteins', type=str,\n",
    "                    help='path to .txt w/ protein names each on new line')\n",
    "\n",
    "parser.add_argument('-o', metavar='outputname', type=str, default=None,\n",
    "                    help='output filename (default will use timestamp.fa)')\n",
    "\n",
    "parser.add_argument('-g', metavar='pathtogencode', type=str,\n",
    "                    default = 'gencode.v28.pc_transcripts.fa.gz',\n",
    "                    help='path to Gencode protein-coding transcripts .gz')\n",
    "\n",
    "parser.add_argument('-n', metavar='numbases', type=int, default=0,\n",
    "                    help='number of bases (e.g. 200: first 200; -100: last 100')\n",
    "\n",
    "parser.add_argument('--m', action='store_true', default=False,\n",
    "                    help='option to use stored ./CDdict.pkl')\n",
    "\n",
    "parser.add_argument('--c', action='store_true', default=False,\n",
    "                    help='option to use stored ./CDdict.pkl')\n",
    "\n",
    "args = parser.parse_args()  # Parse arguments\n",
    "\n",
    "proteins = args.p\n",
    "outputname = args.o\n",
    "pathtogencode = args.g\n",
    "numbases = args.n\n",
    "CDpkl = args.c\n",
    "if numbases == 0:\n",
    "    numbases = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "print('Beginning program.')\n",
    "tic = timeit.default_timer()\n",
    "import gzip\n",
    "from itertools import islice\n",
    "import mygene # this package gets transcipts IDs from UniProt IDs\n",
    "from  tqdm import tqdm\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from time import localtime, strftime\n",
    "import pickle as pkl\n",
    "import requests\n",
    "import ast\n",
    "import math\n",
    "mg = mygene.MyGeneInfo() # the function used to query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert from Cluster of Differentiation (CD) to UniProt\n",
    "Uniprot IDs are easier to work with and have corresponding Ensembl transcript IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not CDpkl:\n",
    "    print('Querying UniProt for CD conversion table.')\n",
    "    '''\n",
    "    Uniprot has a website to convert CDs into Uniprot IDs.\n",
    "    I assume it is updated frequently; latest release April 2018.\n",
    "    The URL is a text file but has poor delimitters, so parsing is\n",
    "    a bit challenging.\n",
    "    '''\n",
    "    \n",
    "    #  this header is where the table starts.\n",
    "    header = \\\n",
    "    'CD      ' + \\\n",
    "    'Swiss-Prot           ' + \\\n",
    "    'MIM     ' +  \\\n",
    "    'Gene              '  + \\\n",
    "    'Name(s) for the protein\\n'\n",
    "    \n",
    "    dflines = list() #  make a list of lines for DataFrame conversion\n",
    "    take = False\n",
    "    with urllib.request.urlopen('https://www.uniprot.org/docs/cdlist.txt') as a:\n",
    "        for i in [j.decode(\"utf-8\") for j in a.readlines()]: # need to decode the website using utf-8\n",
    "            if i == header:\n",
    "                take = True # start taking the lines\n",
    "                first = True # the first line to take\n",
    "            if take:\n",
    "                if first == True:\n",
    "                    first = False\n",
    "                    dflines.append(['CD Number','Swiss-Prot Entry Name','UniProt ID']) # this will be the first line\n",
    "                    continue\n",
    "                if i == '\\n':\n",
    "                    break\n",
    "                if i[:2] == 'CD': # if the line starts with CD, take it\n",
    "                    # only take the first three columns, all  other  columns \n",
    "                    # contain spaces,  hard to parse\n",
    "                    goodsplit = i.split()[:3] \n",
    "                    if goodsplit[-1] != 'N.A.': # only if the uniprot ID exists\n",
    "                        dflines.append(goodsplit)\n",
    "\n",
    "    # Make a dictionary with the conversions\n",
    "    CDdict = dict(zip([i[0] for i in dflines[1:]],[i[2] for i in dflines[1:]]))\n",
    "    pkl.dump(CDdict,open('CDdict.pkl','wb')) # dump the dictionary for future use\n",
    "else:\n",
    "    CDdict = pkl.load(open('CDdict.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Input gene names\n",
    "Gene names are fed as a text file with a name on each line. Names in the text files can be a gene symbol/alias/CD string, with no spaces, or, if it goes by two names, with a single space between the two names. The text file can also have comments with lines starting with '#' that will be ignored.\n",
    "\n",
    "For example:\n",
    "\n",
    "`#These proteins are for my project`\n",
    "\n",
    "`#Gene protein names start here:`\n",
    "\n",
    "`CD137`\n",
    "\n",
    "`CD69`\n",
    "\n",
    "`ICOS (CD278)`\n",
    "\n",
    "`OX40 (CD134)`\n",
    "\n",
    "`HAVCR2`\n",
    "\n",
    "For now querying will use only the first name. Future plans to make it use the second if no results are returned, or ask for manual input of a UniProt ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get markers from somewhere\n",
    "inputmarks = [i.strip() for i in open('markerlist.txt','r').readlines() if i[0] != '#']\n",
    "marksdict = dict()\n",
    "convmarks = list() # add to a new list\n",
    "\n",
    "for i in inputmarks:  # for  each  marker value\n",
    "    if len(i.split(' ')) == 2:\n",
    "        i = i.split(' ')[0] #  separated by a space\n",
    "    try:\n",
    "        convmarks.append(CDdict[i])\n",
    "    except KeyError:\n",
    "        convmarks.append(i)\n",
    "\n",
    "for i in range(len(inputmarks)):\n",
    "    marksdict[inputmarks[i]] = {'Converted': convmarks[i]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Query the MyGene package\n",
    "\n",
    "This next call queries the mygene package. If duplicate hits are found, all of them will be returned. If no hits are found, that transcript will not appear in the generated FASTA. Note that many duplicates might be returned for  gene symbols/aliases inputs, usually because the gene summary (from Ensembl) inlcudes the name (e.g. its an associated/related protein) or they share an alias. The resulting FASTA files should be manually curated by Uniprot ID to ensure only the correct transcript is present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying 1-38...done.\n",
      "Finished.\n",
      "4 input query terms found dup hits:\n",
      "\t[('CCR10', 3), ('ICOS', 2), ('PD1', 3), ('O75144', 2)]\n"
     ]
    }
   ],
   "source": [
    "print('Querying MyGene.')\n",
    "dictout = mg.querymany(convmarks, scopes=['uniprot','symbol','alias'], \n",
    "                       fields=['ensembl.gene','uniprot'], \n",
    "                       species='human', returnall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Flatten MyGene output\n",
    "The mygene output finds all possible transcript IDs matching the UniProt ID. This is a bit cumbersome because output is either a list, dict, or list of dicts, with some of those even containing lists. It's good though because it generalizes the single UniProt ID and corresponding gene to all possible transcripts generated from that locus. Crossed with the gencode human coding transcripts, only those that actually code proteins will be kept.\n",
    "\n",
    "Flatten the ouput, then put everything into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(genes): # function for flattening\n",
    "    genes = str(genes)\n",
    "    genelist = list()\n",
    "    unilist = list()\n",
    "    for i in range(len(genes)):\n",
    "        if genes[i:i+4] == 'ENSG':\n",
    "            genelist.append(genes[i:i+15])\n",
    "#         input(genes[i:i+15])\n",
    "        if genes[i:i+15] == \"'Swiss-Prot': '\":\n",
    "            unilist.append(genes[i+15:i+21])\n",
    "    return genelist, unilist\n",
    "\n",
    "invmarksdict = {v['Converted']: k for k, v in marksdict.items()}\n",
    "for i in dictout['out']: # the output\n",
    "    ensglist, unilist =  flatten(i) # get list of all the IDs\n",
    "    if 'ENSGs' in marksdict[invmarksdict[i['query']]].keys():\n",
    "        marksdict[invmarksdict[i['query']]]['ENSGs'] += ensglist\n",
    "    else:\n",
    "        marksdict[invmarksdict[i['query']]]['ENSGs'] = ensglist\n",
    "    if 'Uniprot' in marksdict[invmarksdict[i['query']]].keys():\n",
    "        marksdict[invmarksdict[i['query']]]['Uniprot'] += unilist\n",
    "    else:\n",
    "        marksdict[invmarksdict[i['query']]]['Uniprot'] = unilist\n",
    "dflist =  list()\n",
    "for i in marksdict.keys():\n",
    "    for j in marksdict[i]['Uniprot']:\n",
    "        for k in marksdict[i]['ENSGs']:\n",
    "            dflist.append([i,j,k])\n",
    "conversiondf = pd.DataFrame(dflist,columns=['Input','UniProt','ENSG'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Ensembl API\n",
    "The ensembl API can be querried for transcript IDs and transcript support levels (TSL). TSLs may be important for a single gene coding for a single protein that has multiple documented coding transcripts. Those transcripts that have less support (i.e. closer to TSL=5 (TSL=1 is highly supported)) might want to be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying Ensembl API.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conversiondf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-80da6ea2f03a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Querying Ensembl API.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mensgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversiondf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ENSG'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://rest.ensembl.org\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conversiondf' is not defined"
     ]
    }
   ],
   "source": [
    "print('Querying Ensembl API.')\n",
    "take = False\n",
    "ensgs = conversiondf['ENSG'].values\n",
    "server = \"http://rest.ensembl.org\"\n",
    "\n",
    "transcripts =  dict()\n",
    "\n",
    "for i in tqdm(ensgs):\n",
    "    transcripts[i] =  dict()\n",
    "    ext = \"/overlap/id/%s?feature=transcript\" % i\n",
    "    r = requests.get(server+ext, headers={ \"Content-Type\" : \"application/json\"})\n",
    "    if not r.ok:\n",
    "        r.raise_for_status()\n",
    "        sys.exit()\n",
    "    decoded = r.json()\n",
    "    jsondicts = ast.literal_eval(repr(decoded))\n",
    "    for j in jsondicts:\n",
    "        try:\n",
    "            tsl =  j['transcript_support_level'] # catch those that don't have TSL\n",
    "        except:\n",
    "            continue\n",
    "        transcripts[i][j['transcript_id']] = tsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a new FASTA File\n",
    "Comb through the protein-coding human transcripts and catch all transcripts that match those from the provided proteins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing FASTA file.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'outputname' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4e0223dd1379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mnumbases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0moutputname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0moutputname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s_transcripts.fa'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d%H%M\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocaltime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0moutputname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.fa'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputname' is not defined"
     ]
    }
   ],
   "source": [
    "print('Writing FASTA file.')\n",
    "def evensplit(n,split): # quick generator for splitting sequence across 80-char lines\n",
    "    startnext = 0\n",
    "    stoplast = split\n",
    "    for i in range(math.ceil(n/split)):\n",
    "        yield (startnext,stoplast)\n",
    "        startnext = stoplast\n",
    "        stoplast += split\n",
    "        if stoplast > n:\n",
    "            stoplast = n\n",
    "\n",
    "seq = ''\n",
    "numbases = -10\n",
    "if outputname == None:\n",
    "    outputname = '%s_transcripts.fa' % strftime(\"%Y%m%d%H%M\", localtime())\n",
    "elif outputname[-3:] != '.fa':\n",
    "    outputname += '.fa'\n",
    "with gzip.open(pathtogencode, 'rt') as fastas, open(outputname,'w') as file:\n",
    "    lines = islice(fastas, 0, None)\n",
    "    counttxnscripts = 0\n",
    "    for line in lines:\n",
    "        if line[0] == '>':\n",
    "            if seq != '':\n",
    "                if numbases != 'all':\n",
    "                    if numbases < 0:\n",
    "                        seq = seq[numbases:]\n",
    "                    elif numbases > 0:\n",
    "                        seq = seq[:numbases]\n",
    "                    else:\n",
    "                        raise ValueError\n",
    "                for i in evensplit(len(seq),80): # FASTA format recommends no more than 80 chars per line\n",
    "                    file.write(seq[i[0]:i[1]]+'\\n')\n",
    "                seq = ''\n",
    "            take = False\n",
    "            splitline = line.split('|')\n",
    "            ENSG = splitline[1].split('.')[0]  #  get ENSG\n",
    "            ENST = splitline[0].split('.')[0][1:]  #  get ENSG\n",
    "            if ENSG in transcripts.keys():\n",
    "                if ENST in transcripts[ENSG].keys():\n",
    "                    counttxnscripts += 1\n",
    "                    index =  conversiondf.index[conversiondf['ENSG'] == ENSG].tolist()[0]\n",
    "                    newhead = '>' + ('|').join(conversiondf.iloc[index]) + '|%s|' % ENST + \\\n",
    "                    'TSL:%s' % str(transcripts[ENSG][ENST]) + '\\n'\n",
    "#                     if counttxnscripts > 170:\n",
    "#                         input(line)\n",
    "#                         input(newhead)\n",
    "                    file.write(newhead)\n",
    "                    take = True\n",
    "                    continue\n",
    "        if take == True:\n",
    "            seq += line.strip('\\n')\n",
    "toc = timeit.default_timer()\n",
    "print(\"Done. Total program time was %d seconds.\" % (toc - tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Code\n",
    "Code for future iterations of this program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def manualinput(notfoundmarks):\n",
    "#     length = len(notfoundmarks)\n",
    "#     print(\"There are %d marker(s) not found. You can choose to ignore these or input UniProt IDs for some or\" + \\\n",
    "#          \" all of them. To skip all, type \"n\". To input UniProt IDs, type 'y'.\" % length)\n",
    "#     answer = input(\"Manually input UniProt IDs? (y/n) \")\n",
    "#     if answer == 'y':\n",
    "#         print(\"Either type in UniProt ID or '-' to skip.\")\n",
    "#         uniprotlist = list()\n",
    "#         for i in notfoundmarks:\n",
    "#             uniprotlist.append(input('Uniprot ID for %s' % i))\n",
    "#         return uniprotlist\n",
    "#     else:\n",
    "#         return ['-']*length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if len(dictout['missing']) != 0:\n",
    "#     print('Trying other names.')\n",
    "#     for i in dictout['missing']:\n",
    "#         for j in inputmarks:\n",
    "#             if i in j and len(i.split(' ')) == 2:\n",
    "#                 str2 = splitted[1][1:len(splitted[1])-1]\n",
    "#                 singledictout = mg.query(str2,scopes=['uniprot','symbol','alias'], \n",
    "#                                          fields=['ensembl.gene','uniprot'], \n",
    "#                                          species='human', returnall=True)\n",
    "#                 if len(singledictout['missing']) == 1:\n",
    "#                     stillnoluck.append(i)\n",
    "#                 else:\n",
    "#                     dictout['out'].append(singledictout)\n",
    "#             else:\n",
    "#                 stillnoluck.append(i)\n",
    "#     manualuni = manualinput(stillnoluck)\n",
    "#     for k in manualuni:\n",
    "#         if i != '-':\n",
    "#             singledictout = mg.query(k,scopes=['uniprot','symbol','alias'], \n",
    "#                          fields=['ensembl.gene','uniprot'], \n",
    "#                          species='human', returnall=True)\n",
    "#             if len(singledictout['missing']) == 1:\n",
    "#                 print('Provided Uniprot ID %s not found. Skipping.' % k)\n",
    "#             else:\n",
    "#                 dictout['out'].append(singledictout)\n",
    "#         else:\n",
    "#             dictout['out'].append({'query': i'skipped': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# while len(goodscripts) == 0:\n",
    "#                         TSL += 1\n",
    "#                         goodscripts = [i['transcript_id'] for i in jsondicts if int(i['transcript_support_level']) <= TSL]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
